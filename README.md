# ğŸ“š Agentic RAG with Ollama (Offline Chatbot)

This project is an **offline Retrieval-Augmented Generation (RAG) pipeline** that allows you to query your own documents (PDFs, text files, etc.) using **local LLMs via Ollama**.  
It works completely **offline**, ensuring privacy and no API costs.  

---

## âš¡ Features
- ğŸ”’ **Fully offline** â€“ no internet or API key required.  
- ğŸ“‚ **PDF ingestion** â€“ load documents into a vector database.  
- ğŸ” **Semantic search with ChromaDB** â€“ retrieves relevant context.  
- ğŸ§  **Local LLM inference** â€“ powered by [Ollama](https://ollama.com).  
- ğŸ”„ **Customizable models** â€“ easily switch between `llama3`, `phi3`, or others.  
- ğŸ’¬ **Interactive Q&A** â€“ ask questions about your documents in natural language.  

---

## ğŸ› ï¸ Prerequisites
Before running this project, install the following:

1. **Python** â‰¥ 3.10  
2. **Ollama** installed â†’ [Download here](https://ollama.com/download)  
   - After installation, pull a model, e.g.:  
     ```bash
     ollama pull phi3:3.8b
     ```
     or  
     ```bash
     ollama pull llama3:8b
     ```
3. **Pip requirements**:  
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“‚ Project Structure
```
agentic-rag-free/
â”‚â”€â”€ app.py              # Main chatbot app
â”‚â”€â”€ ingest.py           # Script to load & process PDFs into ChromaDB
â”‚â”€â”€ requirements.txt    # Dependencies
â”‚â”€â”€ .venv/              # Virtual environment (optional)
â”‚â”€â”€ .chroma/            # Local ChromaDB persistence
â”‚â”€â”€ data/               # Place your PDF/text files here
â”‚â”€â”€ README.md           # Documentation
```

---

## ğŸš€ Usage

### 1. Clone repo & setup environment
```bash
git clone https://github.com/<your-username>/agentic-rag-free.git
cd agentic-rag-free
python -m venv .venv
source .venv/bin/activate   # (Linux/Mac)
.venv\Scripts\activate      # (Windows)
pip install -r requirements.txt
```

### 2. Add documents
Put your PDFs/text files into the `data/` folder. Example:
```
data/
 â””â”€â”€ sample.pdf
```

### 3. Ingest documents
This step converts documents into embeddings & stores them in **ChromaDB**:
```bash
python ingest.py
```

### 4. Run chatbot
```bash
python app.py
```

You can now **ask questions** and get context-aware answers using your offline LLM.

---

## ğŸ”„ Switching Models
By default, the app uses **llama3**.  
To switch to **phi3:3.8b**, edit `app.py`:

```python
# Change model name
model="phi3:3.8b"
```

Then re-run:
```bash
python app.py
```

---

## ğŸ–¼ï¸ Architecture
Hereâ€™s how the flow works:

![Agentic RAG Pipeline](agentic_rag_pipeline.png)

---

## ğŸ“Œ Example
1. Place a **research paper PDF** inside `data/`.  
2. Run `python ingest.py`.  
3. Ask:
   ```
   Q: Summarize section 2 of my PDF.
   A: ...
   ```

---

## ğŸ“¦ Requirements
- chromadb==0.5.5  
- langchain>=0.2.0  
- pypdf  
- ollama  
- python-dotenv  

(Already listed in `requirements.txt`)  

---

## ğŸ™Œ Contributing
Feel free to fork, open issues, or submit PRs.  

---

## ğŸ“œ License
MIT License â€“ free to use, modify, and share.  
